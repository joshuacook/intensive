{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to the bias-variance tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "The aim of this lecture is to visually explore the concept of bias and variance in modeling and the tradeoff between the two.\n",
    "\n",
    "Below we will be fitting models predicting student morale from the day number in the course. \n",
    "\n",
    "\n",
    "Let's assume that the Bruin gods model student morale as such according to a cubic polynomial interpolated from the following data, one data point for each week:\n",
    "\n",
    "    [20, 30, 35, 18, 6, 3, 12, 35, 44, 53, 62, 73, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../preamble.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "days = np.arange(weeks*7+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Data According to the Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "days = np.arange(weeks*7+1)\n",
    "\n",
    "weekpoints = 7*np.arange(weeks+1)\n",
    "moralepoints = [20, 30, 35, 18, 6, 3, 12, 35, 44, 53, 62, 73, 80]\n",
    "\n",
    "list(zip(weekpoints, moralepoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morale function as Interpolated by the Bruin Gods\n",
    "\n",
    "The `interp1d` function creates an interpolation function for us between the week numbers and morale points. The `kind='cubic'` parameter indicates the smoothing of the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morale_func = interp1d(weekpoints, moralepoints, kind='cubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out the \"true function\" of days predicting morale below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, linewidth=5, color='gold', alpha=0.5, label='true function')\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time', fontsize=20)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can never know this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can only model this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In building a model, we will introduce error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A model will have three kinds of error.\n",
    "\n",
    "$$\\text{Error} = \\text{Error from Bias} + \\text{Error from Variance} + \\text{Unknown Error}$$\n",
    "\n",
    "All three of these are strictly non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal when building a model is to minimize the error from bias _and_ the error from variance.\n",
    "\n",
    "The perfect model would have zero bias and zero variance. In reality, the bias or the variance (typically) increases while the other decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this \"true function\" in different ways:\n",
    "\n",
    "> **1. As morale without individual variance:** the true function is the baseline morale for each time point that all students vary around to some degree. Students' morale at any given time point is this baseline morale plus or minus some individual deviation. \n",
    "\n",
    "When coding this in python, for each \"student\" we take the *true* measure of morale from the morale function, and then to create the individual variance we will add random noise to that starting point. \n",
    "\n",
    "> **2. As morale without measurement error:** maybe all student morale at every timepoint is the same, it's just that our way of measuring morale is unreliable. The true function represents the morale at any timepoint for any student without measurement error.\n",
    "\n",
    "If our measurement tool was perfect (a perfect morale survey) we would measure the same morale for every student at each time point. But if the survey form is broken and randomly changes answers by students, for example, then we are adding noise to the measured morale for each observation. \n",
    "\n",
    "> **3. As an average morale across infinite students:** our measurements of morale vary at each time point for each student, but if we had an infinite number of students and averaged all of their morale measurements across all time points, we would end up with the true function of `morale(time)`. \n",
    "\n",
    "**In each case, we are describing morale as a function of time without error, where each is describing a different source of error:**\n",
    "1. Error resulting from an imperfect relationship between time and morale.\n",
    "- Error resulting from an imperfect ability to measure morale.\n",
    "- Error resulting from an insufficient amount of data to quantify the relationship correctly.\n",
    "\n",
    "We will see that each of these sources of error combined make up the full error in any model we build. Respectively:\n",
    "1. The **bias**.\n",
    "3. The **variance**.\n",
    "2. The **irreducible error**.\n",
    "\n",
    "You will always have error in your models, it just depends how much and what proportion of each type. We will formalize these components of error in a model further down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='generate-students'></a>\n",
    "## Generate a sample of students\n",
    "\n",
    "---\n",
    "\n",
    "Say we have four students: student A, B, C, and D.\n",
    "\n",
    "Each student has had their morale checked 12 different times throughout the course, but not necessarily at the same times.\n",
    "\n",
    "**Below is a function that will generate the days and morale for each student as a dictionary object.**\n",
    "\n",
    "> *Note: here we are using the morale function to get the true morale for a given day, then adding the individual variance and/or measurement error from a normal distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_students(f, days, size=12):\n",
    "    students = {}\n",
    "    for student in ['A','B','C','D']:\n",
    "        daysamp = np.random.choice(days, replace=False, size=size)\n",
    "        morales = f(daysamp) + np.random.normal(0,13,size=size)\n",
    "        students[student] = {'days':daysamp, 'morale':morales}\n",
    "    return students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = make_students(morale_func, days, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='student-a'></a>\n",
    "\n",
    "## Student A's morale over time\n",
    "\n",
    "---\n",
    "\n",
    "Below we can plot student A's morale at each day. The true function is also plotted in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.3, label='true function')\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "            s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a-model'></a>\n",
    "\n",
    "## Build a model for days predicting morale using student A's data\n",
    "\n",
    "---\n",
    "\n",
    "With this student's data, I decide to model the relationship between days and morale with a linear regression.\n",
    "\n",
    "My model is:\n",
    "\n",
    "### $$\\hat{morale} = \\beta_0 + \\beta_1days$$\n",
    "\n",
    "**Construct the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studA_days = students['A']['days']\n",
    "studA_mor = students['A']['morale']\n",
    "\n",
    "Amod = LinearRegression()\n",
    "Amod.fit(studA_days[:, np.newaxis], studA_mor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the modeled relationship between days and morale:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "plt.plot(days, Amod.predict(days[:, np.newaxis]), lw=7., c='darkred', alpha=0.5, label='model')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='total-error'></a>\n",
    "\n",
    "## The total error of a model\n",
    "\n",
    "---\n",
    "\n",
    "As you can see above, our regression line is an imperfect representation of the true function. Furthermore, the regression model fails to perfectly capture the variance of the morale in our sample data. \n",
    "\n",
    "The model is our blueprint for the estimation of morale. We have chosen to estimate morale from simply the number of days that have elapsed in the course. In doing so, we have made an assumption: morale is a linear function of days. \n",
    "\n",
    "**When we talk about the bias-variance tradeoff in modeling, and the bias and variance components of error, it is important to think about this in the context of building our model on many samples of the data.** For example, we take our model parameterization `morale ~ days` and build this on the data for student A, then student B, then student C, then student D, and so on. Think of the \"students\" as random samples of morale and days elapsed from the overall population.\n",
    "\n",
    "**There are three sources of error in a model:**\n",
    "\n",
    "### $$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "### $$E\\Big[\\big(\\hat{f}(x) - f(x)\\big)^2\\Big] = \\big(\\;\\text{E}[\\;\\hat{f}(x)\\;] - f(x)\\;\\big)^2 + \\text{E}\\big[\\;(\\;\\hat{f}(x) - \\text{E}[\\;\\hat{f}(x)\\;])^2\\;\\big] + E\\Big[\\big(y - f(x)\\big)^2\\Big] $$\n",
    "\n",
    "**Where**:\n",
    "\n",
    "- $f(x)$ is the true function of y given the predictors.\n",
    "- $\\hat{f}(x)$ is the estimate of y with the model fit on a random sample of the predictors.\n",
    "- $E\\Big[\\big(\\hat{f}(x) - f(x)\\big)^2\\Big]$ is the average squared error across multiple models fit on different  random samples between the model and the true function.\n",
    "- $\\text{E}[\\;\\hat{f}(x)\\;]$ is the average of estimates for given predictors across multiple models fit on different random samples.\n",
    "- $E\\Big[\\big(y - f(x)\\big)^2\\Big]$ is the average squared error between the true values and the predictions from the true function of the predictors. This is the **irreducible error**.\n",
    "- $\\big(\\;\\text{E}[\\;\\hat{f}(x)\\;] - f(x)\\;\\big)^2$ is the squared error between the average predictions across multiple models fit on different random samples and the prediction of the true function. This is the **bias** (squared).\n",
    "- $\\text{E}\\big[\\;(\\;\\hat{f}(x) - \\text{E}[\\;\\hat{f}(x)\\;])^2\\;\\big]$ is the average squared distance between individual model predictions and the average prediction of models across multiple random samples. This is the **variance**.\n",
    "\n",
    "**The irreducible error is \"noise\" â€“ error in the measurement of our target that cannot be accounted for by our predictors.**\n",
    "- The true function represents the most perfect relationship between predictors and target, but that does not mean that our variables can perfectly predict the target.\n",
    "- The irreducible error can be thought of as the measurement error: variation in the target that we cannot represent.\n",
    "\n",
    "We will go into the bias and variance components individually, in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bias'></a>\n",
    "\n",
    "## Bias\n",
    "\n",
    "---\n",
    "\n",
    "The $bias^2$ is the source of error in our model that represents how *oversimplified* our model is. In our example, we have built a function to predict the morale of students as a linear function of days elapsed. However, we can see from the true function that the relationship between days and morale is not a line. This error is encapsulated in the bias.\n",
    "\n",
    "\n",
    "### $$Bias^2 = \\big(\\;\\text{E}[\\;\\hat{f}(x)\\;] - f(x)\\;\\big)^2$$\n",
    "\n",
    "\n",
    "> **Remember:** we have to think about measuring the bias and variance using different fits of our model across multiple random samples! The bias represents the deviation of the *average predictions across models* from the true function.\n",
    "\n",
    "**What does having a high vs. low bias mean?**\n",
    "- If our models are consistently wrong, then the bias will be large.\n",
    "- Alternatively, if our models are consistently correct then the bias will be small.\n",
    "- Bias will be small if the errors across our models built on random samples and tested using the same predictors are incorrect in different directions that average out close to 0. \n",
    "\n",
    "Linear methods like regression tend to have a high bias because we construct a simplification of the true function.\n",
    "\n",
    "**Below we can plot the error differences between the true function and our model built on student A's data.** \n",
    ">  No matter how many lines we fit on different students, those regression lines are never going to average together into the nonlinear true function! Our average estimate across models will always deviate from the true function somewhere across the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "predictions = Amod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, predictions, lw=7., c='darkred', alpha=0.5, label='model')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "for d in students['A']['days']:\n",
    "    p = Amod.predict(d)\n",
    "    plt.plot([d, d], [p, morale_func(d)], c='black', lw=3.)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time (difference from true function)\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='variance'></a>\n",
    "## Variance\n",
    "\n",
    "---\n",
    "\n",
    "The second component of error in the model is the variance of our predictions. \n",
    "\n",
    "Variance describes the extent to which the individual predictions from models built on different samples (students) deviate from the mean of all the model predictions.\n",
    "\n",
    "### $$\\text{Variance} = \\text{E}\\big[\\;(\\;\\hat{f}(x) - \\text{E}[\\;\\hat{f}(x)\\;])^2\\;\\big]$$\n",
    "\n",
    "**What does having a high vs. low variance mean?**\n",
    "- The variance will be large if, for the same observation, models built on different random samples of the data will produce very different predictions.\n",
    "- Variance is a measure of how *consistent* our model's predictions will be if it were fit on another sample of data. \n",
    "- Variance is low if the data we train the model on has very little effect on the predictions.\n",
    "\n",
    "Note that variance is not a measure of how correct or incorrect the predictions are. It is a measure of how variable they are!\n",
    "\n",
    "---\n",
    "\n",
    "### Measuring more students\n",
    "\n",
    "To better visualise the concept of model variance, lets say we measure a second student, student \"B\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['B']['days'], students['B']['morale'],\n",
    "           s=70, c='steelblue', label='student B', alpha=0.7)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we fit a new model to student B's data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit student b's model\n",
    "studB_days = students['B']['days']\n",
    "studB_mor = students['B']['morale']\n",
    "\n",
    "Bmod = LinearRegression()\n",
    "Bmod.fit(studB_days[:, np.newaxis], studB_mor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between the predictions of morale at the same time points between the model for student A and the model for student B?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "Apred = Amod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Apred, lw=7., c='darkred', alpha=0.5, label='A model')\n",
    "\n",
    "Bpred = Bmod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Bpred, lw=7., c='steelblue', alpha=0.5, label='B model')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['B']['days'], students['B']['morale'],\n",
    "           s=70, c='steelblue', label='student B', alpha=0.7)\n",
    "\n",
    "plt.fill_between(days, Apred, Bpred, color='grey', hatch='//', edgecolor=None)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time (difference between estimates)\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-students'></a>\n",
    "## Bias and variance using 3 models\n",
    "\n",
    "---\n",
    "\n",
    "Below we will assess the morale of 3 different students over the days in the course at different times. \n",
    "\n",
    "We can build these simple `morale ~ time` models for each and plot the regression lines.\n",
    "\n",
    "These models are **high bias and low variance**. This is because there is a considerable amount of difference between the average of the model predictions and the true function, but not a lot of variation in predictions at time points across our models for the 3 students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['B']['days'], students['B']['morale'],\n",
    "           s=70, c='steelblue', label='student B', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['C']['days'], students['C']['morale'],\n",
    "           s=70, c='darkgreen', label='student C', alpha=0.7)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit student C's model\n",
    "studC_days = students['C']['days']\n",
    "studC_mor = students['C']['morale']\n",
    "\n",
    "Cmod = LinearRegression()\n",
    "Cmod.fit(studC_days[:, np.newaxis], studC_mor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "Apred = Amod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Apred, lw=5., c='darkred', alpha=0.5, label='A model')\n",
    "\n",
    "Bpred = Bmod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Bpred, lw=5., c='steelblue', alpha=0.5, label='B model')\n",
    "\n",
    "Cpred = Cmod.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Cpred, lw=5., c='darkgreen', alpha=0.5, label='C model')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['B']['days'], students['B']['morale'],\n",
    "           s=70, c='steelblue', label='student B', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['C']['days'], students['C']['morale'],\n",
    "           s=70, c='darkgreen', label='student C', alpha=0.7)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='increasing-complexity'></a>\n",
    "\n",
    "## Increasing complexity to try and capture the true function\n",
    "\n",
    "---\n",
    "\n",
    "Currently our models predict that morale simply increases over time. \n",
    "\n",
    "Looking at the true function, we can see that there is an increase, then a decrease, and then an increase again. \n",
    "\n",
    "Just modeling with a linear effect of time alone can't fit a curve; there is only one coefficient being multiplied by time to make our morale prediction. We could, however, add more variables created from time such as $time^2$, $time^3$, etc.\n",
    "\n",
    "### $$\\hat{morale} = \\beta_0 + \\beta_1 t + \\beta_2 t^2$$\n",
    "### $$\\hat{morale} = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3 + \\beta_4 t^4$$\n",
    "### $$\\hat{morale} = \\beta_0 + \\beta_1 t + ... + \\beta_{16} t^{16}$$\n",
    "\n",
    "**The plots below show the difference in the fit when you add different numbers of \"polynomial\" time variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_modeler(X, y, degrees):\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees,\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polyfit(X, y, truefunc, degrees=[1,2,4,16], \n",
    "                 student_color='darkred', name='A'):\n",
    "    \n",
    "    # set the plot size\n",
    "    plt.subplots(figsize=(20,2))\n",
    "\n",
    "    # create a plot for each polynomial degree plotted\n",
    "    for i in range(len(degrees)):\n",
    "        ax = plt.subplot(1, len(degrees), i + 1)\n",
    "  \n",
    "        poly_model = polynomial_modeler(X, y, degrees[i])\n",
    "\n",
    "        X_test = np.linspace(1, 84, 200)\n",
    "        plt.plot(X_test, poly_model.predict(X_test[:, np.newaxis]), lw=5.,\n",
    "                 c=student_color, label=\"model\", alpha=0.6)\n",
    "        plt.plot(X_test, truefunc(X_test), lw=5., c='gold', alpha=0.6, label='true function')\n",
    "        plt.scatter(X, y, label=\"Student observations\", c=student_color, s=40)\n",
    "        plt.xlabel(\"days\")\n",
    "        plt.ylabel(\"morale\")\n",
    "        plt.xlim((0, 85))\n",
    "        plt.ylim((-20, 100))\n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        plt.title('Student '+name+\" (degree {})\".format(degrees[i]))\n",
    "        \n",
    "plot_polyfit(students['A']['days'], students['A']['morale'], morale_func,\n",
    "             student_color='darkred', name='A')\n",
    "plot_polyfit(students['B']['days'], students['B']['morale'], morale_func,\n",
    "             student_color='steelblue', name='B')\n",
    "plot_polyfit(students['C']['days'], students['C']['morale'], morale_func,\n",
    "             student_color='darkgreen', name='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='complexity-variance'></a>\n",
    "## Higher complexity means higher variance (and lower bias)\n",
    "\n",
    "---\n",
    "\n",
    "The variance of predictions across our models goes up as we increase the model complexity. This is equivalent to saying that the variance of our model is increasing.\n",
    "\n",
    "Increasing the complexity of the model at the expense of good future predictions is known as \"overfitting\" the data. High variance and overfitting are intrinsically related: if your predictions are inconsistent across samples, you are more likely to make the wrong predictions on future data.\n",
    "\n",
    "Likewise, high bias and underfitting are related. If your model is too basic, it may give very consistent predictions but at the cost of oversimplifying the relationship between the target and predictors.\n",
    "\n",
    "**Below are student A and student B fit with the 16-polynomial time model and the area showing the difference in predictions at time points between them.** Compare this to the area we saw earlier with the single time term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\n",
    "\n",
    "Amod_complex = polynomial_modeler(students['A']['days'], students['A']['morale'], 16)\n",
    "Bmod_complex = polynomial_modeler(students['B']['days'], students['B']['morale'], 16)\n",
    "\n",
    "\n",
    "Apred = Amod_complex.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Apred, lw=7., c='darkred', alpha=0.5, label='A model')\n",
    "\n",
    "Bpred = Bmod_complex.predict(days[:, np.newaxis])\n",
    "plt.plot(days, Bpred, lw=7., c='steelblue', alpha=0.5, label='B model')\n",
    "\n",
    "plt.scatter(students['A']['days'], students['A']['morale'],\n",
    "           s=70, c='darkred', label='student A', alpha=0.7)\n",
    "\n",
    "plt.scatter(students['B']['days'], students['B']['morale'],\n",
    "           s=70, c='steelblue', label='student B', alpha=0.7)\n",
    "\n",
    "plt.fill_between(days, Apred, Bpred, color='grey', hatch='//', edgecolor=None)\n",
    "\n",
    "plt.xlabel('days', fontsize=16)\n",
    "plt.ylabel('morale', fontsize=16)\n",
    "plt.title('Morale over time (16th polynomial)\\n', fontsize=20)\n",
    "plt.xlim([0, 85])\n",
    "plt.ylim([-2500,2500])\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://www.evernote.com/l/AAHdmQq6nBpILYZ9UDiME_0CGaPghh2cEAYB/image.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### An Illustrative Example: Voting Intentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we conduct a phone poll to see how voters in a town will vote in the next election and get these results:\n",
    "\n",
    "| Voting Republican\t| Voting Democratic\t| Non-Respondent | \tTotal |\n",
    "|:-----------------:|:-----------------:|:--------------:|:------:|\n",
    "|                13\t|                16\t|              21|     50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_results = {'voting_republican': 13,\n",
    "                'voting_democratic': 16,\n",
    "                'non-respondent': 21}\n",
    "total = sum([val for key,val in poll_results.items()])\n",
    "total_voting = sum([val for key,val in poll_results.items()\n",
    "                    if key != 'non-respondent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_of_voting_R = poll_results['voting_republican']/float(total_voting)\n",
    "probability_of_voting_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put out our press release that the Democrats are going to win by over 10 points; but, when the election comes around, it turns out they actually lose by 10 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
